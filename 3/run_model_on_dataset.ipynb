{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import argparse\n",
    "import os\n",
    "import pickle\n",
    "import cv2\n",
    "from train_utils import *\n",
    "from models import *\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.utils.data\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "\n",
    "from models import *\n",
    "from dataset import WiderDataset\n",
    "from train_utils import *\n",
    "\n",
    "from detect_utils import detect_faces\n",
    "\n",
    "from config import args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_net(test_mode=\"rnet\", thresh=[0.6, 0.7, 0.7], min_face_size=12):\n",
    "\n",
    "    cuda = True\n",
    "\n",
    "    detectors = [None, None, None]\n",
    "\n",
    "    # load pnet model\n",
    "    pnet = PNet()\n",
    "    if cuda:\n",
    "        pnet = pnet.cuda()\n",
    "    checkpoint = torch.load(\"pnet.pth.tar\")\n",
    "    pnet.load_state_dict(checkpoint['state_dict'])\n",
    "    detectors[0] = pnet\n",
    "    \n",
    "    # load rnet model\n",
    "    if test_mode in [\"rnet\", \"onet\"]:\n",
    "        rnet = PNet()\n",
    "        if cuda:\n",
    "            rnet = rnet.cuda()\n",
    "        checkpoint = torch.load(\"model_best_rnet.pth.tar\")\n",
    "        rnet.load_state_dict(checkpoint['state_dict'])\n",
    "        detectors[1] = rnet\n",
    "\n",
    "    # load onet model\n",
    "    if test_mode == \"onet\":\n",
    "        onet = ONet()\n",
    "        if cuda:\n",
    "            onet = onet.cuda()\n",
    "        checkpoint = torch.load(\"model_best_onet.pth.tar\")\n",
    "        onet.load_state_dict(checkpoint['state_dict'])\n",
    "        detectors[2] = onet\n",
    "        \n",
    "        \n",
    "    dataset = WiderDataset(\n",
    "        os.path.join(args.data, \"anno.txt\"),\n",
    "        os.path.join(args.data, \"WIDER_train/images\"))\n",
    "    \n",
    "    \n",
    "    detections = []\n",
    "    for i, (input, _) in enumerate(dataset):\n",
    "        boxes = detect_faces(detectors, input)\n",
    "        detections.append(boxes)\n",
    "        #break\n",
    "        \n",
    "        if i%100 == 0:\n",
    "            print(\"{} images done\".format(i))\n",
    "            \n",
    "    print(\"saving detections\")\n",
    "    \n",
    "    # dummy detection for debug:\n",
    "    #detections = [np.array([[10, 30, 20, 50, 1.0]]) for _ in range(len(dataset))]\n",
    "        \n",
    "    \n",
    "    if test_mode == \"pnet\":\n",
    "        net = \"rnet\"\n",
    "    elif test_mode == \"rnet\":\n",
    "        net = \"onet\"\n",
    "\n",
    "    save_path = os.path.join(args.data, net)\n",
    "    if not os.path.exists(save_path):\n",
    "        os.mkdir(save_path)\n",
    "    save_file = os.path.join(save_path, \"detections.pkl\")\n",
    "    with open(save_file, 'wb') as f:\n",
    "        pickle.dump(detections, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving detections\n"
     ]
    }
   ],
   "source": [
    "# run model on WIDER dataset and pickle the detections\n",
    "\n",
    "test_net(\"pnet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing 12880 images in total\n",
      "0 images done\n",
      "100 images done\n",
      "200 images done\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-8dbc84a2eddf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mnet_mode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"rnet\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0msave_hard_example\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet_mode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-4-b47da165953d>\u001b[0m in \u001b[0;36msave_hard_example\u001b[0;34m(net, basepath)\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m             \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mim_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m         \u001b[0mdets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_to_square\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0mdets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# unpickle detections and generate training data for rnet/onet\n",
    "\n",
    "net_mode = \"rnet\"\n",
    "save_hard_example(net_mode, args.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from box_utils import *\n",
    "\n",
    "def save_hard_example(net, basepath):\n",
    "\n",
    "    image_dir = os.path.join(basepath, \"WIDER_train/images\")\n",
    "    neg_save_dir = os.path.join(basepath, \"24/negative\")\n",
    "    pos_save_dir = os.path.join(basepath, \"24/positive\")\n",
    "    part_save_dir = os.path.join(basepath, \"/24/part\")\n",
    "    anno_file = os.path.join(basepath, 'anno.txt')\n",
    "    save_path = os.path.join(basepath, net)\n",
    "    \n",
    "    \n",
    "\n",
    "    # load ground truth from annotation file\n",
    "    # format of each line: image/path [x1,y1,x2,y2] for each gt_box in this image\n",
    "    with open(anno_file, 'r') as f:\n",
    "        annotations = f.readlines()\n",
    "\n",
    "    if net == \"rnet\":\n",
    "        image_size = 24\n",
    "    if net == \"onet\":\n",
    "        image_size = 48\n",
    "\n",
    "    im_idx_list = list()\n",
    "    gt_boxes_list = list()\n",
    "    num_of_images = len(annotations)\n",
    "    print(\"processing %d images in total\"%num_of_images)\n",
    "\n",
    "    for annotation in annotations:\n",
    "        annotation = annotation.strip().split(' ')\n",
    "        im_idx = annotation[0]\n",
    "\n",
    "        boxes = list(map(float, annotation[1:]))\n",
    "        boxes = np.array(boxes, dtype=np.float32).reshape(-1, 4)\n",
    "        im_idx_list.append(im_idx)\n",
    "        gt_boxes_list.append(boxes)\n",
    "\n",
    "    f1 = open(os.path.join(save_path, 'pos_%d.txt'%image_size), 'w')\n",
    "    f2 = open(os.path.join(save_path, 'neg_%d.txt'%image_size), 'w')\n",
    "    f3 = open(os.path.join(save_path, 'part_%d.txt'%image_size), 'w')\n",
    "\n",
    "    det_boxes = pickle.load(open(os.path.join(save_path, 'detections.pkl'), 'rb'))\n",
    "    assert len(det_boxes) == num_of_images, \"incorrect amount of detections for ground truths\"\n",
    "    \n",
    "    # index of neg, pos and part face, used as their image names\n",
    "    n_idx = 0\n",
    "    p_idx = 0\n",
    "    d_idx = 0\n",
    "    image_done = 0\n",
    "    for im_idx, dets, gts in zip(im_idx_list, det_boxes, gt_boxes_list):\n",
    "        if image_done % 100 == 0:\n",
    "            print(\"%d images done\"%image_done)\n",
    "        image_done += 1\n",
    "\n",
    "        if dets.shape[0]==0:\n",
    "            continue\n",
    "        img = cv2.imread(os.path.join(image_dir, im_idx))\n",
    "        dets = convert_to_square(dets)\n",
    "        dets[:, 0:4] = np.round(dets[:, 0:4])\n",
    "\n",
    "        for box in dets:\n",
    "            x_left, y_top, x_right, y_bottom, _ = box.astype(int)\n",
    "            width = x_right - x_left + 1\n",
    "            height = y_bottom - y_top + 1\n",
    "\n",
    "            # ignore box that is too small or beyond image border\n",
    "            if width < 20 or x_left < 0 or y_top < 0 or x_right > img.shape[1] - 1 or y_bottom > img.shape[0] - 1:\n",
    "                continue\n",
    "\n",
    "            # compute intersection over union(IoU) between current box and all gt boxes\n",
    "            Iou = IoU(box, gts)\n",
    "            cropped_im = img[y_top:y_bottom + 1, x_left:x_right + 1, :]\n",
    "            resized_im = cv2.resize(cropped_im, (image_size, image_size),\n",
    "                                    interpolation=cv2.INTER_LINEAR)\n",
    "\n",
    "            # save negative images and write label\n",
    "            if np.max(Iou) < 0.3:\n",
    "                # Iou with all gts must below 0.3\n",
    "                save_file = os.path.join(neg_save_dir, \"%s.jpg\"%n_idx)\n",
    "                f2.write(\"%s/negative/%s\"%(image_size, n_idx) + ' 0\\n')\n",
    "                cv2.imwrite(save_file, resized_im)\n",
    "                n_idx += 1\n",
    "            else:\n",
    "                # find gt_box with the highest iou\n",
    "                idx = np.argmax(Iou)\n",
    "                assigned_gt = gts[idx]\n",
    "                x1, y1, x2, y2 = assigned_gt\n",
    "\n",
    "                # compute bbox reg label\n",
    "                offset_x1 = (x1 - x_left) / float(width)\n",
    "                offset_y1 = (y1 - y_top) / float(height)\n",
    "                offset_x2 = (x2 - x_right) / float(width)\n",
    "                offset_y2 = (y2 - y_bottom ) / float(height)\n",
    "\n",
    "                # save positive and part-face images and write labels\n",
    "                if np.max(Iou) >= 0.65:\n",
    "                    save_file = os.path.join(pos_save_dir, \"%s.jpg\"%p_idx)\n",
    "                    f1.write(\"%s/positive/%s\"%(image_size, p_idx) + ' 1 %.2f %.2f %.2f %.2f\\n'%(offset_x1, offset_y1, offset_x2, offset_y2))\n",
    "                    cv2.imwrite(save_file, resized_im)\n",
    "                    p_idx += 1\n",
    "\n",
    "                elif np.max(Iou) >= 0.4:\n",
    "                    save_file = os.path.join(part_save_dir, \"%s.jpg\"%d_idx)\n",
    "                    f3.write(\"%s/part/%s\"%(image_size, d_idx) + ' -1 %.2f %.2f %.2f %.2f\\n'%(offset_x1, offset_y1, offset_x2, offset_y2))\n",
    "                    cv2.imwrite(save_file, resized_im)\n",
    "                    d_idx += 1\n",
    "    f1.close()\n",
    "    f2.close()\n",
    "    f3.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
